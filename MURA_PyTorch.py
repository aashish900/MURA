# -*- coding: utf-8 -*-
"""final

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m3hBwARwbx_bxc211bPEcQq3RvFFYwIH
"""

from google.colab import drive
drive.mount('/content/gdrive')

from os import path
from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag
platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())

accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'

!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision
import torch
print(torch.__version__)
print(torch.cuda.is_available())

ls

cp gdrive/My\ Drive/SOP/Copy\ of\ MURA-v1.1.zip /content

ls

!unzip Copy\ of\ MURA-v1.1.zip

cp "gdrive/My Drive/SOP/MURA_CSV/valid_labeled_studies.csv" /content

cp "gdrive/My Drive/SOP/MURA_CSV/valid_image_paths.csv" /content

cp "gdrive/My Drive/SOP/MURA_CSV/train_labeled_studies.csv" /content

cp "gdrive/My Drive/SOP/MURA_CSV/train_image_paths.csv" /content

ls

import cv2
import datetime as dt
import h5py
import matplotlib.pyplot as plt
import matplotlib.pylab as plb
import numpy as np
import csv
import os
import pandas as pd

ls

#Load path to images
root_Path_CSV = 'valid_image_paths.csv'
#root_Path_CSV = 'MURA-v1.1/train_image_paths.csv'
with open(root_Path_CSV) as csvfile:
    path = csv.reader(csvfile, delimiter = ',')
    
    train_Image_Path = []
    
    for row in path:
        image = row[0]
        train_Image_Path.append(image)

root = ''
path = []
for i in range(len(train_Image_Path)):
    pa = root + train_Image_Path[i]
    path.append(pa)

#Assigning images appropriate labels
new_Labels = []
for i in range (0, len(train_Image_Path)):
    if "positive" in train_Image_Path[i]:
        new_Labels.append(1)
    elif "negative" in train_Image_Path[i]:
        new_Labels.append(0)
    else:
        new_Labels.append(None)

df = pd.DataFrame(np.column_stack([train_Image_Path, new_Labels]), 
                  columns = ['Path', 'Title'])

df_saved = df

df1 = df_saved.sample(frac = 0.4)
df = df1
df1.head()

df=df_saved

# License: BSD
# Author: Sasank Chilamkurthy

from __future__ import print_function, division

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
import copy

plt.ion()   # interactive mode

from torch.utils.data import DataLoader, Dataset

data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        #transforms.Grayscale(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'valid': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

#data_dir = 'adit_data'

class ImageDataset(Dataset):
    """training dataset."""

    def __init__(self, df, transform= None):
        """
        Args:
            df (pd.DataFrame): a pandas DataFrame with image path and labels.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.df = df
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        study_path = self.df.iloc[idx, 0]
        image = pil_loader(study_path)
        #print(self.transform)
        image=self.transform(image)
        label = self.df.iloc[idx, 1]
        
        return (image, label)
        
image_datasets = {x: ImageDataset(df, data_transforms[x])
                  for x in ['valid']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=8,
                                             shuffle=False, num_workers=4)
              for x in ['valid']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['valid']}

df.head()

a = ImageDataset(df, data_transforms['valid'])
b=a.__getitem__(idx=1)
b

b[0]

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

"""##Resnet"""

model_ft = models.resnet101(pretrained=False)
num_ftrs = model_ft.fc.in_features
model_ft.fc = nn.Linear(num_ftrs, 2)

model_ft = model_ft.to(device)

criterion = nn.CrossEntropyLoss()

# Observe that all parameters are being optimized
optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)

# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

#torch.save(model_ft, "gdrive/My Drive/SOP/model15e.pth")
model = torch.load("gdrive/My Drive/SOP/model82%.pth")

"""##Densenet"""



class DenseNet169(nn.Module):
    """Model modified.
    The architecture of our model is the same as standard DenseNet121
    except the classifier layer which has an additional sigmoid function.
    """
    def __init__(self, out_size):
        super(DenseNet169, self).__init__()
        self.densenet169 = torchvision.models.densenet169(pretrained=False)
        num_ftrs = self.densenet169.classifier.in_features
        self.densenet169.classifier = nn.Sequential(
            nn.Linear(num_ftrs, out_size),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.densenet169(x)
        return x

#model_ft = models.densenet201(pretrained='imagenet')
#num_ftrs = model_ft.classifier.in_features
#model_ft.classifier = nn.Linear(num_ftrs, 2)

model_ft = DenseNet169(2)
model_ft = model_ft.to(device)

criterion = nn.CrossEntropyLoss()

# Observe that all parameters are being optimized
optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)

# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

#torch.save(model_ft, "gdrive/My Drive/SOP/model15e.pth")
model = torch.load("gdrive/My Drive/SOP/densenet_169_30_epochs.pth")

import sklearn

!pip install Pillow==4.0.0
!pip install PIL
!pip install image
import PIL.image

from torchvision.datasets.folder import pil_loader

ls

phase = 'valid'
model.eval()   # Set model to evaluate mode

running_loss = 0.0
running_corrects = 0

my_answers=[]
my_labels = []

# Iterate over data.
for inputs, labels in dataloaders['valid']:
    #print(inputs)
    #print(labels)
    inputs = inputs.to(device)
    labels=list(labels)
    
    for i,t in enumerate(labels):
      labels[i]=int(t)
    
    labels=torch.tensor(labels)
    labels = labels.to(device)

    # zero the parameter gradients
    optimizer_ft.zero_grad()

    # forward
    # track history if only in train
    with torch.set_grad_enabled(phase == 'train'):
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        loss = criterion(outputs, labels)
        #print(outputs)
        #print(labels)
        #print(image_datasets['valid'].__getitem__(0))
        ans=[]
        for o in outputs:
          if(o[0]>=o[1]):
            ans.append(0)
          else:
            ans.append(1)
        
        for t in ans:
          my_answers.append(t)
        for t in labels:
          my_labels.append(t)
        
        ans =torch.tensor(ans, device=device)
        
       
   
    # statistics
    running_loss += loss.item() * inputs.size(0)
    running_corrects += torch.sum(ans == labels.data)

epoch_loss = running_loss / dataset_sizes[phase]
epoch_acc = running_corrects.double() / dataset_sizes[phase]
kappa = sklearn.metrics.cohen_kappa_score(my_answers, my_labels, labels=None, weights=None, sample_weight=None)

print('{} Loss: {:.4f} Acc: {:.4f}'.format(
    phase, epoch_loss, epoch_acc))

from sklearn.metrics import cohen_kappa_score
#y_true = [2, 0, 2, 2, 0, 1]
#y_pred = [0, 0, 2, 2, 0, 2]
cohen_kappa_score(my_answers, my_labels)

len(my_answers)

df_small.tail()

df['preds'] = my_answers

df_small = pd.read_csv("gdrive/My Drive/SOP/MURA_CSV/valid_labeled_studies.csv", header=None)

df.values[0]

large=df.values[:,0]
large

small=df_small.values[:,0]
small

r1 = df.values[:,2]

len(r1)

r2 = df_small.values[:,1]
r2

len(r2)

my_small_preds=[]
for i in small:
  sum=0
  count=0
  for ind, j in enumerate(large):
    if j.find(i)==0:
      sum=sum+r1[ind]
      count=count+1
  if(sum/count>=0.5):
    my_small_preds.append(1)
  else:
    my_small_preds.append(0)

    
len(my_small_preds)

df_small['2'] = my_small_preds

df_small.head()

cohen_kappa_score(answers, labels)

from sklearn.metrics import accuracy_score
#y_true = [2, 0, 2, 2, 0, 1]
#y_pred = [0, 0, 2, 2, 0, 2]
accuracy_score(answers, labels)

answers=[]
labels=[]
for i in df_small.values[:,1]:
  labels.append(i)
for i in df_small.values[:,2]:
  answers.append(i)

df_small.head()

df_try=df_small

df_try.columns

index=[]
for ind, row in df_try.iterrows():
  if row[0].find("WRIST")!=-1:
    index.append(1)
#  else:
#    index.append(0)

len(index)

df_wrist=df_try[:237]

df_forearm=df_try[237:370]

df_hand=df_try[370:537]

df_humerus=df_try[537:672]

df_shoulder=df_try[672:866]

df_elbow=df_try[866:1024]
df_finger=df_try[1024:]

df_wrist.values[0]

answers=[]
labels=[]
for i in df_wrist.values[:,1]:
  labels.append(i)
for i in df_wrist.values[:,2]:
  answers.append(i) 

  
  
print(accuracy_score(answers, labels))
cohen_kappa_score(answers, labels)

answers=[]
labels=[]
for i in df_forearm.values[:,1]:
  labels.append(i)
for i in df_forearm.values[:,2]:
  answers.append(i) 

  
  
print(accuracy_score(answers, labels))
cohen_kappa_score(answers, labels)

answers=[]
labels=[]
for i in df_hand.values[:,1]:
  labels.append(i)
for i in df_hand.values[:,2]:
  answers.append(i) 

  
  
print(accuracy_score(answers, labels))
cohen_kappa_score(answers, labels)

answers=[]
labels=[]
for i in df_humerus.values[:,1]:
  labels.append(i)
for i in df_humerus.values[:,2]:
  answers.append(i) 

  
  
print(accuracy_score(answers, labels))
cohen_kappa_score(answers, labels)

answers=[]
labels=[]
for i in df_shoulder.values[:,1]:
  labels.append(i)
for i in df_shoulder.values[:,2]:
  answers.append(i) 

  
  
print(accuracy_score(answers, labels))
cohen_kappa_score(answers, labels)

answers=[]
labels=[]
for i in df_elbow.values[:,1]:
  labels.append(i)
for i in df_elbow.values[:,2]:
  answers.append(i) 

  
  
print(accuracy_score(answers, labels))
cohen_kappa_score(answers, labels)

answers=[]
labels=[]
for i in df_finger.values[:,1]:
  labels.append(i)
for i in df_finger.values[:,2]:
  answers.append(i) 

  
  
print(accuracy_score(answers, labels))
cohen_kappa_score(answers, labels)

